#wallerlab
wallerlab_unet:
  type: unet
  # bn_eps: 0.0001
  num_dec_conv: 3
  enc_filters : [24, 64, 128, 256, 512]
  output_activation: #tanh

#flatnet
unet32:
  type: unet
  enc_filters : [32, 64, 128, 256]
  maxpool: false
  intermediate_nodes: true # flatnet + phatnet
  first_kernel_size: 7 # phlatnet
  # last_conv_filter: 12
  depth_space: true
  output_activation: #tanh

unet64:
  type: unet
  enc_filters : [64, 128, 256, 512]
  last_conv_filter: 12
  output_activation: #tanh

unet128:
  type: unet
  enc_filters : [128, 256, 512, 1024]
  # last_conv_filter: 12
  output_activation: #tanh

######################################
###### Experimental models ###########
######################################
# names : 
# 1.  vnet_2d
# 2.  att_unet_2d
# 3.  unet_plus_2d
# 4.  unet_3plus_2d
# 5.  r2_unet_2d
# 6.  resunet_a_2d
# 7.  u2net_2d
# 8.  transunet_2d
# 9.  swin_unet_2d

vnet_2d:
  type: unet_plus
  args:
    filter_num: [16, 32, 64, 128, 256]
    # for 3 colors rgb
    # n_labels: 3
    res_num_ini: 1 
    res_num_max: 3
    activation: "ReLU" # PReLU
    output_activation: #tanh
    batch_norm: true
    pool: false # downsampling with a convolutional layer instead of maxpooling
    unpool: "bilinear"


att_unet_2d:
  type: unet_plus
  args:
    filter_num: [64, 128, 256, 512]
    # for 3 colors rgb
    # n_labels: 3
    stack_num_down: 2
    stack_num_up: 2
    activation: "ReLU"
    atten_activation: "ReLU"
    attention: "add"
    output_activation: #tanh
    batch_norm: true
    pool: true #true for maxpool, false for strided conv + batch norm + activation
    unpool: "bilinear"
    # backbone: 'VGG16'
    # weights: 'imagenet'


unet_plus_2d:
  type: unet_plus
  args:
    filter_num: [64, 128, 256, 512]
    # for 3 colors rgb
    # n_labels: 3
    stack_num_down: 2
    stack_num_up: 2
    activation: "ReLU"
    output_activation: #tanh
    batch_norm: true
    pool: true #true for maxpool, false for strided conv + batch norm + activation
    unpool: "bilinear"
    deep_supervision: false


unet_3plus_2d:
  type: unet_plus
  args:
    filter_num_down: [64, 128, 256, 512]
    # for 3 colors rgb
    # n_labels: 3
    filter_num_skip: 'auto'
    filter_num_aggregate: 'auto'
    stack_num_down: 2
    stack_num_up: 2
    activation: "ReLU"
    output_activation: #tanh
    batch_norm: true
    pool: true #true for maxpool, false for strided conv + batch norm + activation
    unpool: "bilinear"
    deep_supervision: false


r2_unet_2d:
  type: unet_plus
  args:
    filter_num: [64, 128, 256, 512]
    # for 3 colors rgb
    # n_labels: 3
    stack_num_down: 2
    stack_num_up: 2
    activation: "ReLU"
    output_activation: #tanh
    batch_norm: true
    pool: true #true for maxpool, false for strided conv + batch norm + activation
    unpool: "bilinear"
    recur_num: 2



resunet_a_2d:
  type: unet_plus
  args:
    filter_num: [32, 64, 128, 256, 512, 1024]
    dilation_num: [1, 3, 15, 31]
    aspp_num_down: 256
    aspp_num_up: 128
    # for 3 colors rgb
    # n_labels: 3
    # stack_num_down: 2
    # stack_num_up: 2
    activation: "ReLU"
    output_activation: #tanh
    batch_norm: true
    pool: true #true for maxpool, false for strided conv + batch norm + activation
    unpool: "bilinear"
    # deep_supervision: true

u2net_2d:
  type: unet_plus
  args:
    # for 3 colors rgb
    # n_labels: 3
    # stack_num_down: 2
    # stack_num_up: 2
    filter_num_down: [64, 128, 256, 512]
    # filter_num_up: [64, 64, 128, 256] 
    # filter_mid_num_down: [32, 32, 64, 128]
    # filter_mid_num_up: [16, 32, 64, 128]
    # filter_4f_num: [512, 512]
    # filter_4f_mid_num: [256, 256]
    activation: "ReLU"
    output_activation: #tanh
    batch_norm: true
    pool: true #true for maxpool, false for strided conv + batch norm + activation
    unpool: "bilinear"
    deep_supervision: false
    # backbone: 'VGG16'
    # weights: 'imagenet'


transunet_2d:
  type: unet_plus
  args:
    filter_num: [64, 128, 256, 512]

    stack_num_down: 2
    stack_num_up: 2
    embed_dim: 256
    num_mlp: 256
    num_heads: 4
    num_transformer: 4
    activation: 'ReLU'
    mlp_activation: 'GELU'
    output_activation: #tanh
    batch_norm: True
    pool: true #true for maxpool, false for strided conv + batch norm + activation
    unpool: 'bilinear'
    name: 'transunet'

swin_unet_2d:
  type: unet_plus
  args:
    filter_num_begin: 64
    n_labels: 3
    depth: 4
    stack_num_down: 2
    stack_num_up: 2
    patch_size: [2, 2]
    num_heads: [4, 8, 8, 8]
    window_size: [4, 2, 2, 2]
    num_mlp: 256
    output_activation: #tanh
    shift_window: True
    name: 'swin_unet'

# model_name: vnet_2d --> not working
# model_name: att_unet_2d --> interesting
# model_name: unet_plus_2d
# model_name: unet_3plus_2d #--> is really big
# model_name: r2_unet_2d # --> seems quite big: only in MB
# model_name: resunet_a_2d # --> is really big
# model_name: u2net_2d # --> is really big, but works, low lpips
# model_name: transunet_2d # --> is really big
# model_name: swin_unet_2d # --> is really big



