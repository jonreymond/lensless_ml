defaults:
  # - dataset: flatnet_dataset
  # - dataset: phlatnet_dataset
  - dataset : wallerlab_dataset
  
  - model: default_models
  - _self_
# will load specs of wallerlab_unet in default_models
# model_name: unet128_sep
# model_name: unet128_gen
# model_name: wallerlab_unet
model_name: unet64


test: True

# if record output terminal: if error, only the error file is shown: 
# do not use for debugging
record_output: True

resize_input: False
resize_input_height: 520
resize_input_width: 620

# GAN
use_discriminator: False

# camera inversion
use_camera_inversion: True
random_init: False


camera_inversion_non_separable:
  type: "non_separable"
  activation: linear
  # gamma: 20000 # Monakhova
  gamma: 50000 # phlatnet
  pad: False


# don't change
distributed_gpu: False

weight_pruning: False
weight_clustering: False
clustering_params:
  number_of_clusters: 128
  preserve_sparsity: False

# Quantization Aware Training
QAT: True
# QAT_scheme: "QAT" 
QAT_scheme: "PQAT" # pruning + QAT
# QAT_scheme: "CQAT" # clustering + QAT
# QAT_scheme: "PCQAT" # pruning + clustering + QAT



workers: 32
# queue_max_size: 32

batch_size: 8
#validation set percentage
validation_split: 0.2
epochs: 1
# to be used to test the train pipeline : will only train on a subset of the dataset


# device_conv: cuda    # "cuda" or "cpu", not yet implemented for cpu
  
greyscale: False

save: True

use_tensorboard: True
# to get the number of batches processed by tensorboard in each epoch for runtime analysis
# may induce runtime overhead if set too high
tensorboard_profile_batch: 20


load_pretrained : True
load_pretrained_optimizer: True
load_pretrained_lr : False
# flatnet
# pretrained_model_path : /root/jreymond/lensless_ml/outputs/2023-05-10/07-02-20/tensorflow/checkpoints/
# phlatnet
# pretrained_path : /root/jreymond/lensless_ml/outputs/2023-05-15/12-14-26/tensorflow/checkpoints/
# pretrained_path : /home/jreymond/lensless_ml/outputs/2023-06-14/16-23-11/tensorflow/checkpoints/
#best wallerlab set
# pretrained_path_pb : /root/jreymond/lensless_ml/outputs/2023-06-21/07-10-23/tensorflow/models/overal_model.pb
# pretrained_path_pb : /root/jreymond/lensless_ml/outputs/2023-06-21/tensorflow/models/overal_model.pb

# pretrained_path_pb : /home/jreymond/lensless_ml/outputs/2023-06-23/07-02-26/tensorflow/models/overal_model.pb


# gen_unet64, perceptual_model.pb, overal_model.pb, camera_inversion_model.pb
# pruned model
# pretrained_path_pb: /home/jreymond/lensless_ml/outputs/2023-06-26/18-50-57/tensorflow/models/overal_model.pb
pretrained_path_pb: /home/jreymond/lensless_ml/outputs/2023-06-26/18-50-57/tensorflow/models/gen_unet64_pruned.pb

model_weights_path: 


use_crop: True

seed: 1   # Note : will not result in a deterministic output if use cuda device
verbose: 1

optimizer:
  # name of optimizer technique desired
  identifier: "adam"
  learning_rate: 0.0005


loss:
  mse:
    weight: 1
    # either "" or number: additive factor to update the weight of the loss
    additive_factor: "" #-0.05
  lpips:
    # either vgg or alex
    type_model: vgg
    weight: 1.2
    # either "" or number: additive factor to update the weight of the loss
    additive_factor: "" #0.05
  
metric:
  mse:
   # static weight: used when computing metric, useful to know global loss to store best model
    weight: 1
  lpips:
    type_model: vgg
    weight: 1.2
  psnr:
    weight: 0
  ssim:
    weight: 0

lr_reducer:
  # either None if no rl_reducer callback, or reduce_lr_on_plateau, or learning_rate_scheduler
  type: reduce_lr_on_plateau
  # type: learning_rate_scheduler

  reduce_lr_on_plateau:
    monitor: 'val_total'
    factor: 0.70
    patience: 3
    # 6e-8
    min_lr: 0.00000006
    verbose: 1

  learning_rate_scheduler:
    epochs_interval: 5
    factor: 0.5
    min_lr: 0.00000006




discriminator:
  label_smoothing:
    fake_range: [0.0, 0.01]
    true_range: [0.7, 0.75]
  optimizer:
    args:
      identifier: "adam"
      learning_rate: 0.0005
    
    # Will have the same learning rate as the generator optimizer
    copy_gen_lr: True
    use_lr_reducer: False

    lr_reducer:
      type: reduce_lr_on_plateau
      
      reduce_lr_on_plateau:
        monitor: 'val_total'
        factor: 0.5
        patience: 4
        # 1e-7
        min_lr: 0.0000001
        verbose: 1
      learning_rate_scheduler:
        scheduler: "TODO : define function def scheduler(epoch, lr): returning updated lr"
        verbose: 1

  weight: 0.6
  model:
    filters:     [64, 128, 128, 256]
    # strides:     [2,  2,   2,   1] #flatnet
    strides:     [1,  2,   1,   1] #phlatnet
    kernel_size: [3,  3,   3,   3]
    activation: "swish"
    # else will use batchnorm
    use_groupnorm: True
    #num for groupnorm
    num_groups: 4
    sigmoid_output: False




hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}/tensorflow
