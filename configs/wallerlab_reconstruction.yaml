defaults:
  - dataset: wallerlab_dataset
  - model: default_models

use_camera_inversion: False
#will load specs of wallerlab_unet in default_models
model_name: wallerlab_unet
workers: 8
queue_max_size: 32

batch_size: 8
validation_split: 0.2
epochs: 5

device_conv: cuda    # "cuda" or "cpu"
  
greyscale: False

save: True
model_path: stored_models
use_tensorboard: True
tensorboard_path: "tensorboard_results"
# to get the number of batches processed by tensorboard in each epoch for runtime analysis
# may induce runtime overhead if set too high
tensorboard_profile_batch: 50

temp_store_path: "temp"

use_crop: True

seed: 1   # Note : will not result in a deterministic output if use cuda device


optimizer:
  # name of optimizer technique desired
  identifier: "adam"
  learning_rate: 0.01


loss:
  mse:
    weight: 1
    # either "" or number: additive factor to update the weight of the loss
    additive_factor: "" #-0.05
  lpips:
    # either vgg or alex
    model: alex
    weight: 1.6
    # either "" or number: additive factor to update the weight of the loss
    additive_factor: "" #0.05
  
metric:
  mse:
   # static weight: used when computing metric, useful to know global loss to store best model
    weight: 1
  lpips:
    model: alex
    weight: 1.2

lr_reducer:
  # either None if no rl_reducer callback, or reduce_lr_on_plateau, or learning_rate_scheduler
  type: reduce_lr_on_plateau

  reduce_lr_on_plateau:
    monitor: 'val_total'
    factor: 0.5
    patience: 4
    # 6e-8
    min_lr: 0.00000006
    verbose: 1

  learning_rate_scheduler:
    scheduler: "TODO : define function def scheduler(epoch, lr): returning updated lr"
    verbose: 1

use_discriminator: True

discriminator:
  optimizer:
    identifier: "adam"
    learning_rate: 0.01
    use_lr_reducer: True

    lr_reducer:
      type: reduce_lr_on_plateau
      reduce_lr_on_plateau:
        monitor: 'val_total'
        factor: 0.5
        patience: 4
        # 6e-8
        min_lr: 0.00000006
        verbose: 1
      learning_rate_scheduler:
        scheduler: "TODO : define function def scheduler(epoch, lr): returning updated lr"
        verbose: 1

  weight: 0.6
  model:
    filters:     [64, 128, 128, 256]
    strides:     [1,  2,   1,   1]
    kernel_size: [3,  3,   3,   3]
    activation: "swish"
    # else will use batchnorm
    use_groupnorm: False
    #num for groupnorm
    num_groups: 4



